Lab 1 labels 和 nodeSelector

使用以下范例，创建实例文件
nano katacoda.yaml

apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: katacoda
  name: katacoda
spec:
  replicas: 3
  selector:
    matchLabels:
      app: katacoda
  strategy: {}
  template:
    metadata:
      labels:
        app: katacoda
    spec:
      containers:
      - image: katacoda/docker-http-server
        name: docker-http-server
        resources: {}

创建deployment
kubectl apply -f katacoda.yaml 

观察pod
kubectl get pods -o wide
 *每个节点都有一个katacoda，master节点暂时还没有

给node3打标签
kubectl label node node3 proxy=enable

使用以下范例，更新katacoda
nano katacoda3.yaml

apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: katacoda
  name: katacoda
spec:
  replicas: 3
  selector:
    matchLabels:
      app: katacoda
  strategy: {}
  template:
    metadata:
      labels:
        app: katacoda
    spec:
      nodeSelector: # 根据标签匹配调度
        proxy: enable
      containers:
      - image: katacoda/docker-http-server
        name: docker-http-server
        resources: {}

更新deployment
kubectl apply -f katacoda3.yaml

观察pod
kubectl get pods -o wide
 *node2节点上的pod被终止了

查看node3节点详情
kubectl describe nodes node3
  *特别留意labels字段

删除node3的标签
kubectl label node node3 proxy-

再次查看node3节点详情
kubectl describe nodes node3
  *特别留意labels字段

再次观察pod
kubectl get pods -o wide
 *现有的pod还在，符合定义

扩展katacoda的副本数量
kubectl scale deployment katacoda --replicas=4

再次观察pod
kubectl get pods -o wide
 *现有的pod还在，但是新增的pod始终处于pending状态

查看pending的pod
kubectl describe pod katacoda-6f88f95457-z7bvs
 *显示目前没有可用的node "2 node(s) didn't match Pod's node affinity/selector"

清理deployment
kubectl delete -f katacoda3.yaml 

Lab 2 taint 和 tolerations

使用范例创建yaml
nano katacoda.yaml

apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: katacoda
  name: katacoda
spec:
  replicas: 3
  selector:
    matchLabels:
      app: katacoda
  strategy: {}
  template:
    metadata:
      labels:
        app: katacoda
    spec:
      containers:
      - image: katacoda/docker-http-server
        name: docker-http-server
        resources: {}

运行deployment
kubectl apply -f katacoda.yaml 

查看pod列表
kubectl get pod -o wide
  *找到当前负载较高的节点，比如node3

给node3打污点
kubectl taint node node3 aa=bb:NoExecute

查看节点taints
kubectl describe node node3

查看pod列表
kubectl get pod -o wide
  *可以看到该节点上的pod已经被清空

使用以下范例，增加容忍，更新deployment，
nano katacoda2.yaml

apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: katacoda
  name: katacoda
spec:
  replicas: 3
  selector:
    matchLabels:
      app: katacoda
  strategy: {}
  template:
    metadata:
      labels:
        app: katacoda
    spec:
      tolerations:  #增加容忍
      - key: "aa"
        operator: "Equal"
        value: "bb"
        effect: "NoExecute"
      containers:
      - image: katacoda/docker-http-server
        name: docker-http-server
        resources: {}

更新配置
kubectl apply -f katacoda2.yaml

查看pod列表，确认pod在node3节点上重建
kubectl get pods -o wide

删除现有污点
kubectl taint node node3 aa-

查看pod列表
kubectl get pods -o wide


Lab 3  drain 和 uncordon
尝试清空node2
kubectl drain node2
  *查看报错

清空node2 
kubectl drain node2  --ignore-daemonsets

查看节点信息
kubectl get node -o wide
  *node2的status Ready,SchedulingDisabled

查看pod列表
kubectl get pods -o wide
  *pod全部被迁移到node3

扩展katacoda副本数
kubectl scale deployment katacoda --replicas=6

查看pod列表
kubectl get pods -o wide
  *新增负载全在node3上

恢复node2 可调度
kubectl uncordon node2

扩展katacoda副本数
kubectl scale deployment katacoda --replicas=8

查看pod列表
kubectl get pods -o wide
  *新增pod会调度到node2上

收缩katacoda副本数
kubectl scale deployment katacoda --replicas=4

查看pod列表
kubectl get pods -o wide
  *实现node的负载平衡了

清理deployment
kubectl delete -f katacoda2.yaml


Lab 4 使master能够承载工作负载

查看master taints
kubectl describe node node1
 *特别留意 node-role.kubernetes.io/master

删除master污点，使其能承载工作负载
kubectl taint node node1 node-role.kubernetes.io/master-

运行deployment
kubectl apply -f katacoda.yaml

查看pod列表，确认pod运行在三个节点上
kubectl get pods -o wide

清理deployment
kubectl delete -f katacoda.yaml 

使用以下范例创建deamonsets
nano katacoda-daemonsets.yaml

apiVersion: apps/v1
kind: DaemonSet
metadata:
  labels:
    app: katacoda-daemonsets
  name: katacoda-daemonsets
spec:
  selector:
    matchLabels:
      app: katacoda-daemonsets
  template:
    metadata:
      labels:
        app: katacoda-daemonsets
    spec:
      containers:
      - image: katacoda/docker-http-server
        name: docker-http-server
        resources: {}

运行daemonset
kubectl apply -f katacoda-daemonsets.yaml 

查看pod列表
kubectl get pod -o wide

清理daemonset
kubectl delete -f katacoda-daemonsets.yaml 

查看pod列表
kubectl get pod -o wide
  *曲终人散

恢复master的taint
kubectl taint node node1 node-role.kubernetes.io/master:NoSchedule

再次运行daemonset
kubectl apply -f katacoda-daemonsets.yaml 

查看pod列表
kubectl get pod -o wide
  *node1上没有deamonsets pod


Lab 5 部署能够运行在master上的daemonset

使用以下范例，增加针对master的容忍，更新deployment，
nano katacoda-daemonsets2.yaml 

apiVersion: apps/v1
kind: DaemonSet
metadata:
  labels:
    app: katacoda-daemonsets
  name: katacoda-daemonsets
spec:
  selector:
    matchLabels:
      app: katacoda-daemonsets
  template:
    metadata:
      labels:
        app: katacoda-daemonsets
    spec:
      containers:
      - image: katacoda/docker-http-server
        name: docker-http-server
        resources: {}
      tolerations: # 增加针对master的容忍
      - key: node-role.kubernetes.io/master
        operator: Exists
        effect: NoSchedule

创建deployment
kubectl apply -f katacoda-daemonsets2.yaml

观察pod分布
kubectl get pod -o wide

清理daemonset
kubectl delete -f katacoda-daemonsets2.yaml 


备注：
删除所有master污点，使其能承载工作负载
kubectl taint nodes --all node-role.kubernetes.io/master-
