Lab 1 安装metrics-server

使用附录中的范例创建metrics-server的文件

安装metrics-server
kubectl apply -f metrics-server.yaml

查看pod
kubectl get pod -n kube-system

kubectl get pod -n kube-system | grep metrics-server

测试功能
kubectl top node

kubectl top pod

#kubectl top pod -A

kubectl top pod | sort -k3 -nr 

kubectl top pod | sort -k3 -nr | head -1 

kubectl top pod | sort -k3 -nr | head -1 | awk '{print $2}'

kubectl top pod | sort -k3 -nr | head -1 | awk '{print $2}' >/tmp/memtop.txt
  *考试真题


Lab 2 HPA
# 修改kube-controller-manager.yaml，增加启动参数
nano /etc/kubernetes/manifests/kube-controller-manager.yaml 

    - --horizontal-pod-autoscaler-use-rest-clients=true
    - --horizontal-pod-autoscaler-downscale-delay=5m0s
    - --horizontal-pod-autoscaler-upscale-delay=20s
    - --horizontal-pod-autoscaler-sync-period=10s

# 修改好之后，观察kube-controller-manager的自动重启
kubectl get pod -n kube-system | grep kube-controller-manager

使用范例创建podinfo 
nano podinfo.yaml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: podinfo
spec:
  selector:
    matchLabels:
      app: podinfo
  replicas: 2
  template:
    metadata:
      labels:
        app: podinfo
      annotations:
        prometheus.io/scrape: "true"
    spec:
      containers:
        - name: podinfod
          image: stefanprodan/podinfo:2.0.0
          imagePullPolicy: Always
          #command:
          #  - ./podinfo
          #  - -port=9898
          #  - -logtostderr=true
          #  - -v=2
          volumeMounts:
            - name: metadata
              mountPath: /etc/podinfod/metadata
              readOnly: true
          ports:
            - containerPort: 9898
              protocol: TCP
          readinessProbe:
            httpGet:
              path: /readyz
              port: 9898
            initialDelaySeconds: 1
            periodSeconds: 2
            failureThreshold: 1
          livenessProbe:
            httpGet:
              path: /healthz
              port: 9898
            initialDelaySeconds: 1
            periodSeconds: 3
            failureThreshold: 2
          resources:
            requests:
              memory: "32Mi"
              cpu: "1m"
            limits:
              memory: "256Mi"
              cpu: "100m"
      volumes:
        - name: metadata
          downwardAPI:
            items:
              - path: "labels"
                fieldRef:
                  fieldPath: metadata.labels
              - path: "annotations"
                fieldRef:
                  fieldPath: metadata.annotations


创建pod
kubectl apply -f podinfo.yaml

查看pod，重点关注数量
kubectl get pod

使用范例创建podinfo HPA
nano podinfo-hpa.yaml

---
apiVersion: autoscaling/v2beta2
kind: HorizontalPodAutoscaler
metadata:
  name: podinfo
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: podinfo
  minReplicas: 2
  maxReplicas: 10
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 80
    - type: Resource
      resource:
        name: memory
        target:
          type: AverageValue
          averageValue: 200M

启用HPA
kubectl apply -f podinfo-hpa.yaml

查看HPA 关注target和replica
kubectl get hpa

kubectl get hpa -o yaml

查看pod，重点关注数量，查看水平扩展效果
kubectl get pod -o wide | grep podinfo

查看pod资源使用
kubectl top pod | grep podinfo

查看扩展过程
kubectl describe hpa podinfo

查看deployment
kubectl describe deploy podinfo

清理
kubectl delete -f podinfo-hpa.yaml 
kubectl delete -f podinfo.yaml

Lab 3 LimitRange

使用以下范例创建limitrange定义文件

nano limitrange.yaml

apiVersion: v1
kind: LimitRange
metadata:
  name: cpu-min-max-demo-lr
  namespace: default
spec:
  limits: 
  - type: Container
    max: # 上限
      cpu: "800m"
      memory: 1Gi
    default: # 默认的limits
      cpu: "800m"
      memory: "500Mi"
    defaultRequest: # 默认的request
      cpu: "500m"
      memory: "500Mi"
    min: # 下限
      cpu: "200m"
      memory: "500Mi" 
  - type: PersistentVolumeClaim
    max:
      storage: 2Gi
    min:
      storage: 1Gi

创建limitrange
kubectl apply -f limitrange.yaml

查看limitrange
kubectl get limitrange

kubectl describe limitrange cpu-min-max-demo-lr

使用以下命令行创建pod用例
kubectl run lrpod1 --image=katacoda/docker-http-server

查看pod
kubectl describe pod lrpod1
  *侧重观察Limits和Requests配置信息

使用以下范例创建pod

nano lrpod2.yaml

apiVersion: v1
kind: Pod
metadata:
  name: lrpod2
  namespace: default
spec:
  containers:
  - image: katacoda/docker-http-server
    name: lrpod2
    resources:
      limits:
        cpu: 800m
        memory: 2Gi
      requests:
        cpu: 100m
        memory: 500Mi

创建pod
kubectl apply -f lrpod2.yaml 
  *此处应该有报错
   # Error from server (Forbidden): error when creating "lrpod2.yaml": pods "lrpod2" is forbidden: [minimum cpu usage per Container is 200m, but request is 100m, maximum memory usage per Container is 1Gi, but limit is 2Gi]

修改lrpod2.yaml
nano lrpod2.yaml

apiVersion: v1
kind: Pod
metadata:
  name: lrpod2
  namespace: default
spec:
  containers:
  - image: katacoda/docker-http-server
    name: lrpod2
    resources:
      limits:
        cpu: 800m
        memory: 1Gi # 调整到内存上限以内
      requests:
        cpu: 400m # 调整到cpu下限之上
        memory: 500Mi

再次创建pod
kubectl apply -f lrpod2.yaml 

kubectl describe pod lrpod2
  *侧重观察Limits和Requests配置信息

Lab 2 ResourceQuota

使用以下范例创建ResourceQuota定义文件
nano resourcequota.yaml

apiVersion: v1
kind: ResourceQuota
metadata:
  name: mem-cpu-demo
spec:
  hard: # 强制约束
    requests.cpu: "1"
    requests.memory: 1Gi
    limits.cpu: "2"
    limits.memory: 2Gi

创建resourcequota
kubectl apply -f resourcequota.yaml 

查看resourcequota
kubectl get resourcequota

kubectl describe resourcequota mem-cpu-demo  
  *重点关注Used

创建pod
kubectl run lrpod3 --image=katacoda/docker-http-server
  *观察到exceededquota报错

删除lrpod1
kubectl delete pod lrpod1

再次查看resourcequota
kubectl describe resourcequota mem-cpu-demo  
  *重点关注Used

创建pod
kubectl run lrpod3 --image=katacoda/docker-http-server

清理
kubectl delete pod lrpod3
kubectl delete pod lrpod2
kubectl delete -f limitrange.yaml 
kubectl delete -f resourcequota.yaml 


Lab 3 使用Helm安装redis

安装helm
wget https://chengzhstor.blob.core.windows.net/k8slab/helm-v3.3.0-linux-amd64.tar.gz

tar xf helm-v3.3.0-linux-amd64.tar.gz

mv linux-amd64/helm /usr/bin/

替代安装办法

curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3

chmod 700 get_helm.sh

./get_helm.sh

查看helm版本
helm version

添加仓库并找到redis chart
helm repo add stable https://charts.helm.sh/stable
  *这一步时间可能比较久，请耐心等待

查看已经添加的仓库
helm repo list

搜索仓库有哪些chart
helm search repo stable

更新仓库列表到本地
helm repo update

搜索redis
helm search repo redis

查看redis chart详情
helm show chart stable/redis

查看redis values（values：相当于chart的配置文件）
helm show values stable/redis

redis chart安装/升级/回滚/卸载
建立单master的配置文件
nano only-master.values

## Cluster settings
cluster:
  enabled: false

## Redis pod Security Context
securityContext:
  enabled: false

## Use password authentication
usePassword: true
## Redis password (both master and slave)
password: "admin"

## Redis Master parameters
master:
  persistence:
    enabled: false

--dry-run一下，看看生成出来的yaml文件是否存在问题
helm install redis-demo stable/redis -f ./only-master.values --dry-run

如果没有问题，则进行实际的安装
helm install redis-demo stable/redis -f ./only-master.values

安装成功之后，可以登录redis进行操作，做进一步的校验
redis-cli -h `kubectl get svc redis-demo-master -o=jsonpath="{.spec.clusterIP}"` -a admin
set name zhangsan
get name
info replication
 *可以看到这时候没有slave连接

建立master-slave配置文件
nano master-slave.values

## Cluster settings
cluster:
  enabled: true
  slaveCount: 1

securityContext:
  enabled: false

## Use password authentication
usePassword: true
password: "admin"

## Mount secrets as files instead of environment variables
usePasswordFile: false

## Redis Master parameters
master:
  persistence:
    enabled: false

## Redis Slave properties
slave:
  persistence:
    enabled: false


--dry-run一下，看看生成出来的yaml文件是否存在问题; 由于在系统中已经有redis-demo的release，因此使用upgrade来进行升级
helm upgrade redis-demo stable/redis -f ./master-slave.values --dry-run
helm upgrade redis-demo stable/redis -f ./master-slave.values

检查slave是否安装成功，以及是否同步成功
redis-cli -h `kubectl get svc redis-demo-slave -o=jsonpath="{.spec.clusterIP}"` -a admin
get name

回滚至单master模式,查看部署历史
helm history redis-demo

回滚到对应的单master版本
helm rollback redis-demo 1

再连接slave已经不成功了
redis-cli -h `kubectl get svc redis-demo-slave -o=jsonpath="{.spec.clusterIP}"` -a admin

只有master能连接
redis-cli -h `kubectl get svc redis-demo-master -o=jsonpath="{.spec.clusterIP}"` -a admin

测试一下
get age

卸载redis-demo
helm uninstall redis-demo

Lab 3 使用Helm安装wordpress

在NFS服务器上创建存放pvc的文件夹
mkdir /data/mariadb-pv
mkdir /data/wordpress-pv
chmod 777 -R /data/mariadb-pv
chmod 777 -R /data/wordpress-pv

使用范例创建pv定义
nano blogpv.yaml

apiVersion: v1
kind: PersistentVolume
metadata:
  name: mariadb-pv
spec:
  capacity:
    storage: 8Gi
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  #storageClassName: nfs-csi
  nfs:
    path: /data/mariadb-pv
    server: 192.168.1.202
---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: wordpress-pv
spec:
  capacity:
    storage: 10Gi
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  #storageClassName: nfs-csi
  nfs:
    path: /data/wordpress-pv
    server: 192.168.1.202

创建pv
kubectl apply -f blogpv.yaml

安装chart
helm install wordpress stable/wordpress
查看pod
kubectl get pod

查看服务
kubectl get svc

将wordpress服务发布在30080端口
kubectl patch svc  wordpress  -p '{"spec":{"type": "NodePort"}}'
kubectl patch service wordpress  --type='json' --patch='[{"op": "replace", "path": "/spec/ports/0/nodePort", "value":30080}]'

访问wordprss
http://node1:30080

查询wordkpress用户名和密码
echo Username: user
echo Password: $(kubectl get secret --namespace default wordpress -o jsonpath="{.data.wordpress-password}" | base64 --decode)
  *记录密码

访问wordprss管理界面
http://node1:30080/admin

卸载wordpress
helm uninstall wordpress


Lab 使用Helm安装kubernetes-dashboard
增加repo
helm repo add incubator http://aliacs-k8s-cn-beijing.oss-cn-beijing.aliyuncs.com/app/charts-incubator

部署dashboard
helm fetch incubator/kubernetes-dashboard

tar xf kubernetes-dashboard-2.8.1.tgz

cd kubernetes-dashboard

修改配置文件
nano values.yaml
  *使用NodePort

安装dashboard
helm install kubernetes-dashboard -n kube-system ./

查看pod
kubectl get pod -n kube-system

查看svc
kubectl get svc -n kube-system  
  *关注dashboard的端口号

创建一个sa
kubectl create sa -n kube-system chengzh

给sa赋权
kubectl create clusterrolebinding chengzh@kubernetes --serviceaccount=kube-system:chengzh --clusterrole=cluster-admin

查看该sa
kubectl describe sa chengzh -n kube-system
  *关注token的标识

获取token
kubectl describe secret chengzh-token-d9z5c -n kube-system

eyJhbGciOiJSUzI1NiIsImtpZCI6ImxJWEg4dUREcXZ2WDhQZHdiNFZoR3BVNi1wU0VJbFZrc1B0RGt4RjVaajgifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJjaGVuZ3poLXRva2VuLWQ5ejVjIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6ImNoZW5nemgiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiJmNjNhOTY1MS05NjhjLTRiOTktYmU1MC1hYjU1MjhhNTIwODgiLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6a3ViZS1zeXN0ZW06Y2hlbmd6aCJ9.cvza57s6u8ZyJPDv51_3dCOYlTzwomuhD_M7qHRlYMJvDKyOIOrUGrT__DGO6-OMcDS8B7oSvmEuHtslQa1sULuqrF45SCbVOUalWVo6yetifu7VqnfPehO8o_iPmy1jnSGCyvT63GasjQgkYfAd-zDf8yK7gQNot6FSLtA_n3ruo7Hf2L9GUdiCIBzuQetxR4h6-BU4pxDHg4YPDThLkfQS21ns-qFl5YolQF1NbPe1JpQ54z3ZJDTxYriKg4gODBc8PtIkMv4s5knEFscArEjBh5dwHPNi_sVXOp5UD8i6Eq5DslA38ULb12P1hgoDCOBSMXuqqie4N7Wv3cxrfw

使用该token登录到dashboard
https://nodepublicip:nodeport 

enjoy

Lab 4 安装 kube-prometheus-stack  

创建命名空间
kubectl create ns prom

添加官方repo
helm repo add prometheus-community https://prometheus-community.github.io/helm-charts

获取chart
helm fetch prometheus-community/kube-prometheus-stack  

安装kube-prometheus-stack
tar xf kube-prometheus-stack-34.1.0.tgz  #截至2022-3-18，最新版本是34.1.0

cd kube-prometheus-stack

参照以下步骤修改ingress-nginx镜像源
nano values.yaml

ctrl \

k8s.gcr.io/ingress-nginx

chengzh

y

样例
 image:
        repository: chengzh/kube-webhook-certgen
        tag: v1.1.1
        # sha: "" 注释掉sha
        pullPolicy: IfNotPresent

参照以下步骤修改kube-state-metrics镜像源
nano charts/kube-state-metrics/values.yaml

ctrl \

k8s.gcr.io/kube-state-metrics/kube-state-metrics

chengzh

y

样例
image:
  repository: chengzh/kube-state-metrics
  tag: v2.4.1
  pullPolicy: IfNotPresent

安装release
helm install kube-prometheus-stack -n prom ./

查看pod和svc
kubectl get pod -n prom

kubectl get svc -n prom

将grafana的端口映射到nodeport 31121
kubectl patch svc -n prom kube-prometheus-stack-grafana  -p '{"spec":{"type": "NodePort"}}'
kubectl patch service kube-prometheus-stack-grafana --namespace=prom --type='json' --patch='[{"op": "replace", "path": "/spec/ports/0/nodePort", "value":31121}]'

访问grafana
http://nodepublicip:31121
用户名：admin 
密码：prom-operator

# 卸载release
# helm uninstall kube-prometheus-stack  -n prom

Lab 5 安装ELK
#在每一个节点上执行以下命令修改内核参数
# sudo sysctl -w vm.max_map_count=262144

创建命名空间
kubectl create ns efk

在NFS服务器上创建存放pvc的文件夹
mkdir /data/elasticsearch

设置共享权限
chmod 777 -R /data/elasticsearch

使用以下命令行创建Elasticsearch使用的PV

nano es-pv.yaml

apiVersion: v1
kind: PersistentVolume
metadata:
  name: elasticsearch
spec:
  capacity:
    storage: 30Gi
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  nfs:
    path: /data/elasticsearch
    server: nfs # 此处使用NFS服务器地址

kubectl create -f es-pv.yaml
# kubectl apply -f k8slab/perfmon/es-pv.yaml
# kubectl create -f https://raw.githubusercontent.com/cloudzun/k8slab/main/perfmon/es-pv.yaml

下载Helm仓库源码
wget -O helm-charts.tgz  https://github.com/elastic/helm-charts/archive/7.9.2.tar.gz
tar -zxvf helm-charts.tgz
cd helm-charts-7.9.2

国内替代方案
wget -O helm-charts.tgz  https://chengzhstor.blob.core.windows.net/k8slab/helm-charts-7.9.2.tar.gz
tar -zxvf helm-charts.tgz
cd helm-charts-7.9.2


更改elasticsearch副本数，此处选用最低副本数
nano elasticsearch/values.yaml
  replicas: 1
  minimumMasterNodes: 1

安装elasticsearch
helm install es --namespace=efk ./elasticsearch

修改 filebeat api version
nano filebeat/templates/clusterrole.yaml

apiVersion: rbac.authorization.k8s.io/v1

nano filebeat/templates/clusterrolebinding.yaml 

apiVersion: rbac.authorization.k8s.io/v1

安装filebeat
helm install fb --namespace=efk ./filebeat

修改kibana服务参数，使用nodeprot
nano kibana/values.yaml

  service:
    type: NodePort
    port: 5601
    nodePort: "31126"

安装kibana
helm install kb --namespace=efk ./kibana

查看pod和svc
kubectl get pod -n efk

kubectl get svc -n efk


使用31126端口打开kibana界面进行初始化

卸载服务
helm uninstall es --namespace=efk ./elasticsearch
helm uninstall fb --namespace=efk ./filebeat
helm uninstall kb --namespace=efk ./kibana




备注：
EFK步骤
https://www.cnblogs.com/leozhanggg/p/12700363.html

群集重启后重建elasticsearch步骤
helm uninstall es --namespace=efk ./elasticsearch

kubectl delete pvc elasticsearch-master-elasticsearch-master-0 -n efk

kubectl delete pv elasticsearch 

rm -rf /data/elasticsearch/nodes/

kubectl apply -f es-pv.yaml 

helm install es --namespace=efk ./elasticsearch


选做 Lab Potainer安装 

使用以下范例创建pv定义
nano portainer-pv.yaml

apiVersion: v1
kind: PersistentVolume
metadata:
  name: portainer-pv
  namespace: portainer
  annotations:
    volume.alpha.kubernetes.io/storage-class: "generic"
  labels:
    io.portainer.kubernetes.application.stack: portainer
    app.kubernetes.io/name: portainer
    app.kubernetes.io/instance: portainer
    app.kubernetes.io/version: "ce-latest-ee-2.4.0"
spec:
  capacity:
    storage: 10Gi
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  nfs:
    path: /data/portainer
    server: 192.168.1.202

创建pv
kubectl apply -f portainer-pv.yaml

安装portainer
kubectl apply -n portainer -f https://raw.githubusercontent.com/portainer/k8s/master/deploy/manifests/portainer/portainer.yaml

检查portainer端口
kubectl get svc -n portainer





